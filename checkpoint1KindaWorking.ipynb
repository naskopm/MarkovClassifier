{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda35c08-1f0e-452d-9ce8-6ab558ef37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1151dbda-80b2-4fd9-afb8-e9918b53d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "wordlistStringString = []\n",
    "counter = 0\n",
    "trainingData = []\n",
    "with open(r\"D:\\rockyou.txt\\generatepasswords\\csvRockyou.csv\", 'r', encoding='utf-8') as file:    \n",
    "    # Now read from the 100,000th line onward\n",
    "    for line in file:\n",
    "        # Process each line from here on\n",
    "        wordlistStringString.append(line)\n",
    "        wordlistStringString[counter] = wordlistStringString[counter].replace(\"\\n\", \"\")\n",
    "        counter += 1\n",
    "        if counter > 200000:\n",
    "            trainingData.append(line)\n",
    "            if counter == 400000:\n",
    "                break\n",
    "    \n",
    "\n",
    "labels = []\n",
    "for i in range(200000):\n",
    "    labels.append(1)\n",
    "for i in range(200000):\n",
    "    labels.append(0)\n",
    "test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92205d7-c23f-445d-9b02-675e52e446b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the file generating passwords...\n",
      "loaded the passwords\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "import random\n",
    "# Parameters\n",
    "filename = r\"C:\\Users\\1\\Downloads\\xato-net-10-million-passwords-1000000.txt\"\n",
    "num_rows = 100000  # Number of lines to read from rockyou.txt\n",
    "min_length = 8\n",
    "max_length = 12\n",
    "\n",
    "# Step 1: Load Data\n",
    "with open(filename, 'r', encoding=\"latin-1\") as file:\n",
    "    passwords = [next(file).strip() for _ in range(num_rows)]\n",
    "\n",
    "# Step 2: Build Sparse Transition Matrix on a Word Level\n",
    "# Map each unique word to an index\n",
    "word_list = list(set(passwords))  # Unique words (passwords) in the dataset\n",
    "word_to_index = {word: i for i, word in enumerate(word_list)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "num_words = len(word_list)\n",
    "\n",
    "# Initialize sparse matrix using Dictionary of Keys (DOK) for efficient building\n",
    "transition_counts = dok_matrix((num_words, num_words), dtype=np.int32)\n",
    "\n",
    "# Populate the sparse transition matrix with counts based on word sequences\n",
    "for password in passwords:\n",
    "    words = list(password)  # Treat each character of a password as a \"word\" for now\n",
    "    for i in range(len(words) - 1):\n",
    "        word1, word2 = words[i], words[i + 1]\n",
    "        if word1 in word_to_index and word2 in word_to_index:\n",
    "            transition_counts[word_to_index[word1], word_to_index[word2]] += 1\n",
    "\n",
    "# Convert to CSR format for efficient row slicing\n",
    "transition_counts = transition_counts.tocsr()\n",
    "\n",
    "# Step 3: Normalize Counts to Probabilities\n",
    "for i in range(num_words):\n",
    "    row_sum = transition_counts[i, :].sum()\n",
    "    if row_sum > 0:\n",
    "        transition_counts[i, :] = transition_counts[i, :] / row_sum\n",
    "print(\"loaded the file generating passwords...\")\n",
    "# Step 4: Generate Passwords Using Sparse Transition Matrix\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "passwords = []\n",
    "# Sample list of passwords as training data\n",
    "with open(\"C:\\\\Users\\\\1\\\\Downloads\\\\xato-net-10-million-passwords-1000000.txt\", encoding=\"utf8\") as f:\n",
    "    for i in range(100000):\n",
    "        passwords.append(f.readline().strip())\n",
    "\n",
    "# Join passwords as a single string with a separator (optional)\n",
    "text = \" \".join(passwords)\n",
    "print(\"loaded the passwords\")\n",
    "\n",
    "# Create a Markov model with bigram-level states (state_size=2)\n",
    "model = markovify.Text(text, state_size=2)\n",
    "bigrams = list(model.chain.model.keys())\n",
    "# Function to generate a password by starting from a bigram\n",
    "def generate_password(model, length=12):\n",
    "    # Choose a random bigram from the model's keys to start\n",
    "    bigram = random.choice(bigrams)\n",
    "    password = ''.join(bigram)  # Initialize with the bigram\n",
    "\n",
    "    # Generate the rest of the password\n",
    "    while len(password) < length:\n",
    "        state = tuple(password[-2:])  # Last two characters as the new state\n",
    "        next_char_options = model.chain.model.get(state)\n",
    "\n",
    "        if next_char_options:\n",
    "            # Fast selection of the most frequent next character\n",
    "            next_char = max(next_char_options, key=next_char_options.get)\n",
    "            password += next_char[-1]  # Append only the last character\n",
    "        else:\n",
    "            # If we hit a dead-end, restart from a new random bigram\n",
    "            bigram = random.choice(list(model.chain.model.keys()))\n",
    "            password += ''.join(bigram)[:2]  # Append part of new bigram as fallback\n",
    "\n",
    "    return password[:length]  # Ensure exact length\n",
    "\n",
    "\n",
    "# Generate a batch of passwords\n",
    "\n",
    "test = [generate_password(model, length=12) for _ in range(200000)]\n",
    "# Generate a batch of passwords\n",
    "\n",
    "for i in range(len(test)):\n",
    "    trainingData.append(test[i])\n",
    "for i in range(len(trainingData)):\n",
    "    trainingData[i] = trainingData[i].replace(\"\\n\", \"\")\n",
    "\n",
    "print(len(trainingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d671b-2b78-459e-a44f-e7ea95a83c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zipped_data = list(zip(trainingData, labels))\n",
    "print(zipped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659a8c88-7df5-439e-9b71-0d2183e3279a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming 'wordlistStringString' is your training data and 'labels' is the corresponding label array\u001b[39;00m\n\u001b[0;32m     10\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m HashingVectorizer(\n\u001b[0;32m     11\u001b[0m     analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000000\u001b[39m,  \u001b[38;5;66;03m# Adjust the number of features (e.g., 2^20)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, ),  \u001b[38;5;66;03m# Use unigrams and bigrams\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32,  \u001b[38;5;66;03m# Use float32 for lower memory usage\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 16\u001b[0m X_sparse \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished vectorizing features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Define the function to compute dense featuresimport numpy as np\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:907\u001b[0m, in \u001b[0;36mHashingVectorizer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    890\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform a sequence of documents to a document-term matrix.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m        Document-term matrix.\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\users\\1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:854\u001b[0m, in \u001b[0;36mHashingVectorizer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    851\u001b[0m     )\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m--> 854\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_ngram_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hasher()\u001b[38;5;241m.\u001b[39mfit(X, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\1\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:515\u001b[0m, in \u001b[0;36m_VectorizerMixin._validate_ngram_range\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_ngram_range\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check validity of ngram_range parameter\"\"\"\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     min_n, max_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m min_n \u001b[38;5;241m>\u001b[39m max_m:\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for ngram_range=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower boundary larger than the upper boundary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    520\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range)\n\u001b[0;32m    521\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Assuming 'wordlistStringString' is your training data and 'labels' is the corresponding label array\n",
    "vectorizer = HashingVectorizer(\n",
    "    analyzer='char',\n",
    "    n_features=2000000,  # Adjust the number of features (e.g., 2^20)\n",
    "    ngram_range=(1, ),  # Use unigrams and bigrams\n",
    "    dtype=np.float32,  # Use float32 for lower memory usage\n",
    "    )\n",
    "X_sparse = vectorizer.fit_transform(trainingData)\n",
    "print(\"finished vectorizing features\")\n",
    "# Define the function to compute dense featuresimport numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def compute_dense_features_vectorized(data):\n",
    "    # Calculate the lengths of each word\n",
    "    lengths = np.array([len(word) for word in data])\n",
    "    \n",
    "    # Calculate lowercase, uppercase, and digit counts for each word\n",
    "    lowercases = np.array([sum(c.islower() for c in word) for word in data])\n",
    "    uppercases = np.array([sum(c.isupper() for c in word) for word in data])\n",
    "    digits = np.array([sum(c.isdigit() for c in word) for word in data])\n",
    "\n",
    "    # Calculate the ratios\n",
    "    digit_ratios = np.divide(digits, lengths, where=lengths != 0)\n",
    "    letter_ratios = np.divide(lowercases + uppercases, lengths, where=lengths != 0)\n",
    "    uppercase_ratios = np.divide(uppercases, lengths, where=lengths != 0)\n",
    "    lowercase_ratios = np.divide(lowercases, lengths, where=lengths != 0)\n",
    "\n",
    "    # Stack all the features into a single matrix\n",
    "    return np.column_stack((lengths, lowercases, uppercases, digits, digit_ratios, letter_ratios, uppercase_ratios, lowercase_ratios))\n",
    "print(\"computing dense features\")\n",
    "# Compute dense features for the entire list at once\n",
    "dense_features = compute_dense_features_vectorized(trainingData)\n",
    "\n",
    "# Convert dense features to a sparse matrix format\n",
    "dense_features_sparse = csr_matrix(dense_features)\n",
    "print(len(labels))\n",
    "print(len(trainingData))\n",
    "print(zip(trainingData, labels))\n",
    "# Combine the sparse hashed text features with the dense numeric features\n",
    "X_combined = hstack([X_sparse, dense_features_sparse])\n",
    "print(\"finished computing features, training the model...\")\n",
    "# Continue with model training and evaluation as before\n",
    "classifier = LogisticRegression(  # Optimized for large datasets and sparse data\n",
    "    penalty='l1',           # L2 regularization\n",
    "    solver='saga',          # Optimized for large, sparse datasets\n",
    "    max_iter=100000,          # Higher iterations for convergence\n",
    "    C=1.0,                  # Regularization strength (adjust as needed)\n",
    "    random_state=42  \n",
    ")\n",
    "classifier.fit(X_combined, labels)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = classifier.predict(X_combined)\n",
    "report = classification_report(labels, predictions)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ca0ce-6fc0-42d4-8f2b-1a9faecb8101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "generatedPasswords = []\n",
    "numOfPasswords = 1000000\n",
    "print(f\"generatiing {numOfPasswords} passwords\")\n",
    "for i in range(numOfPasswords):\n",
    "    generatedPasswords.append(generate_password(model, length=12))\n",
    "print(f\"generated: {numOfPasswords} passwords\")\n",
    "    \n",
    "# Vectorize training data and compute dense features\n",
    "print(\"vectorizing Data\")\n",
    "vectorizer = HashingVectorizer(\n",
    "    analyzer='char',\n",
    "    n_features=2000000,  # Adjust the number of features (e.g., 2^20)\n",
    "    ngram_range=(1, 6),  # Use unigrams and bigrams\n",
    "    dtype=np.float32,  # Use float32 for lower memory usage\n",
    "    )\n",
    "X_train_sparse = vectorizer.fit_transform(generatedPasswords)\n",
    "print(\"finished vectorizing\")\n",
    "print(\"computing features\")\n",
    "dense_train_features = compute_dense_features_vectorized(generatedPasswords)\n",
    "dense_train_sparse = csr_matrix(dense_train_features)\n",
    "\n",
    "print(dense_train_sparse.shape)\n",
    "print(X_train_sparse.shape)\n",
    "# Combine sparse n-grams with dense features for test data\n",
    "X_test_combined = hstack([X_train_sparse, dense_train_sparse])\n",
    "\n",
    "\n",
    "# Predict using the classifier\n",
    "print(\"finished computing features. predicting passwords...\")\n",
    "predictions = classifier.predict(X_test_combined)\n",
    "\n",
    "# Output the results\n",
    "count_ones = np.sum(predictions == 1)\n",
    "count_zeros = np.sum(predictions == 0)\n",
    "print(\"Number of 1s:\", count_ones)\n",
    "print(\"Number of 0s:\", count_zeros)\n",
    "\n",
    "# Print predicted potential passwords\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction == 1:\n",
    "        print(test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea07a83-4b26-44da-9a58-acbf526061bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1779-65a3-4388-8364-a1423df8a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = final_df.drop(columns=[\"password\", \"bigram_ids\", \"trigram_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeded7b-fa53-4ea8-8a3f-d0c3a96d1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3722ccf-047a-4f8a-bd0a-c8be4402c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63d24c-0e76-4203-868b-d36b5bba01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa775c83-5666-4312-8743-3a748d4cdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fac563-e55d-4975-bc2a-2731021f8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    length = random.randint(8, 12)\n",
    "       # Ensure the first character is a letter, either uppercase or lowercase\n",
    "    first_char = random.choice(string.ascii_letters[:26] + string.ascii_letters[26:])  # First letter can be upper or lower\n",
    "    \n",
    "    # Generate the rest of the string with lowercase letters and digits\n",
    "    remaining_chars = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length - 1))\n",
    "    \n",
    "    wordlistStringString.append(first_char + remaining_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4254954-3d49-4feb-8232-8824b8797c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData = process_passwords(randomS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc280e-592a-490d-b1df-fd2194fa969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126e592-71f8-4150-a193-2a2572973d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tryingLabels = kmeans.predict(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5e72d-8e8b-4a35-b744-771f6da2094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(tryingLabels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11998811-2c84-4b3e-9336-08c122f5c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2ea5b-2368-47dd-a6a6-01dd4886cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06645e33-6582-429b-bc39-f46a9840aef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
